{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "\n",
    "print(sys.path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['/Users/dlq/Pytorch_record', '/Users/dlq/Pytorch_record', '', '/opt/homebrew/Cellar/pyside@2/5.15.2/lib/python3.9/site-packages', '/Users/dlq/miniforge3/envs/pytorch/lib/python39.zip', '/Users/dlq/miniforge3/envs/pytorch/lib/python3.9', '/Users/dlq/miniforge3/envs/pytorch/lib/python3.9/lib-dynload', '/Users/dlq/miniforge3/envs/pytorch/lib/python3.9/site-packages', '/Users/dlq/miniforge3/envs/pytorch/lib/python3.9/site-packages/IPython/extensions', '/Users/dlq/.ipython']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 初始化一个张量"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 直接从列表数据生成\n",
    "data: a list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(x_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 使用Numpy数组创建"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "print(x_np)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 使用另一个已有张量"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f'x_ones:\\n{x_ones}')\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f'x_rand:\\n{x_rand}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_ones:\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "x_rand:\n",
      "tensor([[0.9053, 0.2512],\n",
      "        [0.8981, 0.7536]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 使用固定值或随机值\n",
    "shape是张量维度的元组，它决定了输出张量的维度。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'rand_tesnor: \\n {rand_tensor} \\n')\n",
    "print(f'ones_tensor: \\n {ones_tensor} \\n')\n",
    "print(f'zeros_tensor: \\n {zeros_tensor} \\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rand_tesnor: \n",
      " tensor([[0.3155, 0.1040, 0.0080],\n",
      "        [0.0521, 0.7990, 0.7086]]) \n",
      "\n",
      "ones_tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "zeros_tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Tensor的属性\n",
    "张量的属性用于描述张量本身的尺寸、数据类型以及变量储存的位置。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print('Shape of tensor:', tensor.shape)\n",
    "print('Datatype of tensor:', tensor.dtype)\n",
    "print('device of tensor:', tensor.device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "device of tensor: cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 对Tensor的操作\n",
    "对张量的操作超过100种，包括算术、线性代数、矩阵操作（转置、索引、切片），这些操作既可以在CPU上完成，也可以在GPU上完成，在GPU上进行运算的速度更快一些。\n",
    "一般而言，张量是在CPU上创建的，我们需要使用.to命令来将张量移动到GPU中。但是跨设备进行大尺度张量的复制操作是花费很多时间和内存的。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we move our tensors to the GPU fi available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "对张量进行操作（类似Numpy）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row:', tensor[0])\n",
    "print('First column:', tensor[:, 0])\n",
    "print('Last column:', tensor[:, -1])\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "张量级联：可以使用torch.cat来将一系列张量沿着指定的维度进行级联，实际上torch.stack也可以用来对张量进行拼接，但是二者的关键区别在于：返回的Tensor的维数是否会发生改变。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "cat = torch.cat((a, b), dim=1) # dim指定拼接维度，默认值为0\n",
    "stack = torch.stack((a, b), dim=1)\n",
    "print('shape of tensor with torch.cat:', cat.shape)\n",
    "print('shape of tensor with torch.stack:', stack.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of tensor with torch.cat: torch.Size([2, 6])\n",
      "shape of tensor with torch.stack: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "矩阵运算：以矩阵乘法和点乘为例"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:, 1] = 0\n",
    "\n",
    "# 这里将要计算两个张量之间的矩阵乘法，y1,y2,y3具有相同的值\n",
    "y1 = tensor @ tensor.T # .T表示转置\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print('y1:', y1)\n",
    "print('y2:', y2)\n",
    "print('y3:', y3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y1: tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y2: tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y3: tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tensor = torch.rand(4, 4)\n",
    "\n",
    "# 这里将要计算两个张量之间的点乘运算，z1,z2,z3具有相同的值\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print('z1:', z1)\n",
    "print('z2:', z2)\n",
    "print('z3:', z3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "z1: tensor([[0.9925, 0.0130, 0.7903, 0.1408],\n",
      "        [0.1539, 0.1473, 0.4970, 0.0156],\n",
      "        [0.3221, 0.3825, 0.1823, 0.5702],\n",
      "        [0.8966, 0.1325, 0.4991, 0.0054]])\n",
      "z2: tensor([[0.9925, 0.0130, 0.7903, 0.1408],\n",
      "        [0.1539, 0.1473, 0.4970, 0.0156],\n",
      "        [0.3221, 0.3825, 0.1823, 0.5702],\n",
      "        [0.8966, 0.1325, 0.4991, 0.0054]])\n",
      "z3: tensor([[0.9925, 0.0130, 0.7903, 0.1408],\n",
      "        [0.1539, 0.1473, 0.4970, 0.0156],\n",
      "        [0.3221, 0.3825, 0.1823, 0.5702],\n",
      "        [0.8966, 0.1325, 0.4991, 0.0054]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "单元素张量：如果我们有一个单元素的张量，那么我们可以用item()将其转化为python中的纯数值"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg, type(agg))\n",
    "print(agg_item, type(agg_item))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(16.) <class 'torch.Tensor'>\n",
      "16.0 <class 'float'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "就地操作：将结果存储到操作数中的操作被就地调用。这些操作会带有_符号。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[6., 6., 6., 6.],\n",
      "        [6., 6., 6., 6.],\n",
      "        [6., 6., 6., 6.],\n",
      "        [6., 6., 6., 6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 与Numpy之间的联系\n",
    "CPU中的张量可以和Numpy数组共享底层的存储地址，对其中一个进行修改将会同时修改另一个。\n",
    "### 4.1 张量 ——> Numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "t = torch.ones(5)\n",
    "print('t:', t)\n",
    "\n",
    "n = t.numpy()\n",
    "print('n:', n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# 修改张量中的元素，查看Numpy数组是否会跟着变动\n",
    "t.add_(1)\n",
    "print('t:', t)\n",
    "print('n:', n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Numpy ——> Tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print('t:', t)\n",
    "print('n:', n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "028035cf017d87bb851db29ebd50d359435970863d4fb5b203bd6f380b6099ff"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}